# ğŸš€ AI Code Review Agent  
### _A Transformer-based Automated Reviewer for GitHub Pull Requests_

The **AI Code Review Agent** is a deep-learning system designed to automatically review GitHub pull requests using a **custom dataset**, **adaptive diff extraction**, and a **fine-tuned FLAN-T5 model**.  
It generates concise, structured review suggestions following this format:

Suggested Review:
<rewritten comment>

Reasoning:
<brief explanation>

Fix:
Replace these lines:
<old code>

With this:
<new code>

---

## âœ¨ Features

- ğŸ” **Diff-aware review generation** (changed-line detection + context extraction)  
- ğŸ§© **Adaptive hunk grouping** (3â€“7 lines, stops at next `@@`)  
- âœï¸ **Consistent PR-review output structure**  
- ğŸ› ï¸ **Fine-tuned FLAN-T5 Small** using custom instruction format  
- ğŸ“¦ **Notebook-driven training pipeline** (local + Kaggle-compatible)  
- ğŸ”§ **Supports `.keras` model saving**  
- ğŸš¦ **Beginner-friendly modular workflow**  

---

## ğŸ“ Project Structure

```text
C:\Users\yogan\OneDrive\Desktop\ai-code-reviewer
â”‚
â”œâ”€â”€ .ipynb_checkpoints/
â”‚   Auto-generated Jupyter metadata
â”‚
â”œâ”€â”€ 1.data Loading.ipynb
â”‚   Raw dataset loading and streaming reader
â”‚
â”œâ”€â”€ 2.data wrangling.ipynb
â”‚   Cleaning, hunk parsing, change detection
â”‚
â”œâ”€â”€ 3.formatting.ipynb
â”‚   Prompt construction and target formatting
â”‚
â”œâ”€â”€ 4.train_val_test_split.ipynb
â”‚   Adaptive splitting and sanity checks
â”‚
â”œâ”€â”€ 5.tokenization_padding_batch_testing.ipynb
â”‚   Tokenizer, padding, collate_fn batch validation
â”‚
â”œâ”€â”€ code-review-model-training (kaggle_trained).ipynb
â”‚   Kaggle GPU/TPU training version
â”‚
â”œâ”€â”€ code-review-model-training(full_scale_local_run).ipynb
â”‚   Local full-scale training loop
â”‚
â”œâ”€â”€ local_model_testing.ipynb
â”‚   Inference tests using saved .keras model
â”‚
â”œâ”€â”€ app/
â”‚   (Future) PR automation endpoint/scripts
â”‚
â”œâ”€â”€ model_download_v3/
â”‚   Saved trained model (.keras)
â”‚
â”œâ”€â”€ venv/
â”‚   Virtual environment
â”‚
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ (All dataset files excluded intentionally)

```
## ğŸ§  Model Architecture

- **Backbone:** FLAN-T5 Small  
- **Type:** Encoderâ€“Decoder (seq2seq)  
- **Training Objective:** Next-token prediction over structured review format  
- **Input:**  
  - Changed code lines  
  - Small context window (before + after lines)  
  - System prompt defining output structure  
- **Output:**  
  - Suggested Review  
  - Reasoning  
  - Fix instructions with minimal diff replacement  

---

## ğŸ‹ï¸ Training Workflow

### 1ï¸âƒ£ Data Preparation  
Performed across Notebooks 1â€“4:  
- Stream JSONL â†’ extract PR diffs  
- Skip extremely large hunks  
- Group lines adaptively  
- Build model-ready prompt/label pairs  
- Split into train/val/test

### 2ï¸âƒ£ Tokenization  
- SentencePiece tokenizer (T5 default)  
- Dynamic padding  
- Masking labels as `-100` for loss to avoid NaN issues  

### 3ï¸âƒ£ Training  
- Runs on **GPU** (P100) or **TPU v5e-8** (Kaggle)  
- `.keras` checkpointing  
- Gradient clipping, LR warmup, AMP options  

### 4ï¸âƒ£ Testing  
- Local inference notebook  
- Ensure structure formatting correctness  

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/<your-username>/ai-code-reviewer
cd ai-code-reviewer
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
â–¶ï¸ How to Run Training
bash
Copy code
# Launch Jupyter
jupyter notebook
Open one of the training notebooks:

code-review-model-training (kaggle_trained).ipynb

code-review-model-training(full_scale_local_run).ipynb
```
## ğŸ”® Roadmap
 Real-time PR integration via GitHub Webhooks

 Lightweight FastAPI endpoint for review generation

 Frontend UI for visualizing code diffs

 Expand training to multi-language diff datasets

 Add rule-based fallback reviewer

### ğŸ“ License
This project is open-source under the MIT License.

### ğŸ’¬ Contact
If you want help expanding the repo, optimizing training, or deploying to production, feel free to reach out!
