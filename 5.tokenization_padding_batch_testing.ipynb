{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc716cb-3214-4686-9a9e-ca1d33f2e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ex = json.loads(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9f1ca-0542-4be8-a69f-8749e53aeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fb588-971a-4903-b26e-b556f9f3c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_tok = 512\n",
    "max_target_tok = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25def3-14e0-478d-8127-7637e9b044e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs = tokenizer(ex['prompt'], truncation = True, max_length = max_input_tok)\n",
    "enc_target = tokenizer(ex['completion'], truncation = True, max_length = max_target_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4486cf-9468-47a0-aa9b-023fa84e6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor(enc_inputs['input_ids'])\n",
    "attention_mask = torch.tensor(enc_inputs['attention_mask'])\n",
    "labels = torch.tensor(enc_target['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859aa9e-37b8-4c0f-99c7-eb14ead43735",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4634c7-1e2f-4b50-a588-6a8315e9fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"input_ids\" : input_ids.unsqueeze(0),\n",
    "    \"attention_mask\" : attention_mask.unsqueeze(0),\n",
    "    \"labels\" : labels.unsqueeze(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee29ae-9e78-41fc-a589-21a198df40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5p-220m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ea5dc-b8f8-4136-8610-eb0a40d14f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    padded_inputs = tokenizer.pad({\n",
    "        \"input_ids\" : input_ids,\n",
    "        \"attention_mask\" : attention_mask\n",
    "    }, return_tensors = \"pt\")\n",
    "    \n",
    "    max_len = padded_inputs[\"input_ids\"].size(1)\n",
    "    padded_label_list = []\n",
    "    \n",
    "    for lbl in labels:\n",
    "        lbl = list(lbl)  # ensure list\n",
    "        if len(lbl) > max_len:\n",
    "            lbl = lbl[:max_len]\n",
    "        else:\n",
    "            lbl = lbl + [tokenizer.pad_token_id] * (max_len - len(lbl))\n",
    "        padded_label_list.append(lbl)\n",
    "\n",
    "    padded_label_input_ids = torch.tensor(padded_label_list, dtype=torch.long)\n",
    "    padded_label_input_ids[padded_label_input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "    \"input_ids\" : padded_inputs['input_ids'],\n",
    "    \"attention_mask\" : padded_inputs['attention_mask'],\n",
    "    \"labels\" : padded_label_input_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b890c4c-2d45-4028-aeb2-9a88b97da025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class PRDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_input = 512, max_target = 256):\n",
    "        self.data = [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\") if l.strip()]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input = max_input\n",
    "        self.max_target = max_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "\n",
    "        enc_in = self.tokenizer(ex['prompt'], truncation = True,padding = False, max_length = self.max_input)\n",
    "        enc_out = self.tokenizer(ex['completion'], truncation = True,padding = False, max_length = self.max_target)\n",
    "\n",
    "        return {\n",
    "        'input_ids' : enc_in['input_ids'],\n",
    "        'attention_mask' : enc_in['attention_mask'],\n",
    "        'labels' : enc_out['input_ids']\n",
    "        }\n",
    "\n",
    "train_dataset = PRDataset('train_data.jsonl', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2576a-7a21-46ed-9f36-987681c3f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07811ce-9387-44c8-99cf-bb5efdd259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a9218-8cc0-4906-8cb2-e3c1b7aeacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "inputs = batch['input_ids']\n",
    "labels = batch['labels']\n",
    "masks = batch.get('attention_mask')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "optimizer.zero_grad()\n",
    "outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "\n",
    "logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "print(\"one batch loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a35f08-aca7-483f-af6f-2b51845d6cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
